{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['senter']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import nltk\n",
    "import spacy\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "import random\n",
    "from resiliparse.parse.html import HTMLTree\n",
    "from resiliparse.parse.encoding import detect_encoding\n",
    "from resiliparse.extract.html2text import extract_plain_text\n",
    "from get_training_query_term_recall import calculate_doc_term_recall\n",
    "from passage_extraction_util import passage_calculate_doc_term_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1279425it [04:47, 4454.40it/s]\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "\n",
    "with open('./reddit-tomt-submissions.jsonl') as f, gzip.open('./reddit-tomt-submissions-with-links.jsonl.gz', 'wt') as output_file:\n",
    "    for l in tqdm(f):\n",
    "        l = json.loads(l)\n",
    "        if 'links_on_answer_path' in l and len(l['links_on_answer_path']) > 0:\n",
    "            output_file.write(json.dumps(l) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "326458it [00:57, 5700.86it/s]\n"
     ]
    }
   ],
   "source": [
    "with gzip.open('./reddit-tomt-submissions-with-links.jsonl.gz') as f:\n",
    "    entries = []\n",
    "    for l in tqdm(f):\n",
    "        l = json.loads(l)\n",
    "        del l['comments']\n",
    "        entries.append(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_urls(urls):\n",
    "    ret = []\n",
    "    for url in urls:\n",
    "        url = url.split(')')[0]\n",
    "        url = url.split('(')[-1]\n",
    "        ret.append(url)        \n",
    "    return ret\n",
    "        \n",
    "assert ['https://www.reddit.com/r/woahdude/comments/66wqhv/true_cyan_color_illusion/'] == clean_urls(['https://www.reddit.com/r/woahdude/comments/66wqhv/true_cyan_color_illusion/'])\n",
    "assert ['https://www.reddit.com/r/woahdude/comments/66wqhv/true_cyan_color_illusion/'] == clean_urls(['https://www.reddit.com/r/woahdude/comments/66wqhv/true_cyan_color_illusion/)'])\n",
    "assert ['https://www.reddit.com/r/woahdude/comments/66wqhv/true_cyan_color_illusion/'] == clean_urls(['[text](https://www.reddit.com/r/woahdude/comments/66wqhv/true_cyan_color_illusion/)nochmehrtext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4955/3802094837.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mget_htmls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'links_on_answer_path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'http://www.louissachar.com/Wayside.htm'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'http://www.louissachar.com/Wayside.htm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'websites/www.louissachar.com/Wayside.htm'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mget_htmls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'links_on_answer_path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'http://www.youtube.com/watch?v=NU75uz0b8EU'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'http://www.youtube.com/watch?v=NU75uz0b8EU'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'websites/www.youtube.com/watch?v=NU75uz0b8EU/index.html'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ToDo: I remember that I saw that the parsing above does not work for xy.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_htmls(entry):\n",
    "    links = clean_urls(entry['links_on_answer_path'])\n",
    "    \n",
    "    ret = dict()\n",
    "    for link in links:\n",
    "        html_file = f\"websites/{link.split('://')[-1]}\"\n",
    "        if exists(html_file):\n",
    "            for suffix in ['','/index.html']:\n",
    "                if os.path.isfile(html_file + suffix):\n",
    "                    ret[link] = html_file + suffix\n",
    "                    break\n",
    "            if link not in ret:\n",
    "                raise ValueError('todo')\n",
    "        \n",
    "    return ret\n",
    "\n",
    "assert get_htmls({'links_on_answer_path': ['http://www.louissachar.com/Wayside.htm']}) == {'http://www.louissachar.com/Wayside.htm': 'websites/www.louissachar.com/Wayside.htm'}\n",
    "assert get_htmls({'links_on_answer_path': ['http://www.youtube.com/watch?v=NU75uz0b8EU']}) == {'http://www.youtube.com/watch?v=NU75uz0b8EU': 'websites/www.youtube.com/watch?v=NU75uz0b8EU/index.html'}\n",
    "assert False, 'ToDo: I remember that I saw that the parsing above does not work for xy.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_page(reddit_entry):\n",
    "    if get_htmls(reddit_entry):\n",
    "        return\n",
    "    \n",
    "    current_directory = os.getcwd()\n",
    "    links = clean_urls(reddit_entry['links_on_answer_path'])\n",
    "    timestamp = int(reddit_entry['solved_utc'])\n",
    "    time = datetime.datetime.utcfromtimestamp(timestamp)\n",
    "    time_start = (time - datetime.timedelta(days=365)).strftime(\"%Y%m%d%H%M%S\")\n",
    "    time_end = (time + datetime.timedelta(days=365)).strftime(\"%Y%m%d%H%M%S\")\n",
    "    for link in links:\n",
    "        command = [\n",
    "        \"docker\",\n",
    "        \"run\",\n",
    "        \"-v\",\n",
    "        f\"{current_directory}/websites:/websites\",\n",
    "        \"hartator/wayback-machine-downloader\",\n",
    "        \"--exact-url\",\n",
    "        \"--from\",\n",
    "        time_start,\n",
    "        \"--to\",\n",
    "        time_end,\n",
    "        \"--maximum-snapshot\",\n",
    "        \"2\",\n",
    "        link\n",
    "        ]\n",
    "        subprocess.run(command, check=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'websites/www.louissachar.com/Wayside.htm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4955/2470409340.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'websites/www.louissachar.com/Wayside.htm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'\\nLouis Sachar —  Wayside School Book Series\\n\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'websites/www.louissachar.com/Wayside.htm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'main'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLouis Sachar —  Wayside School Book Series\\n\\n \\n\\nTo purchase any of Louis's books visit your local bookseller or make your selection online:\\n\\nSideways Stories from\\nWayside School\\n\\n\\xa0\\n\\nWayside School\\nis Falling Down\\n\\n\\xa0\\n\\nWayside School\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4955/2470409340.py\u001b[0m in \u001b[0;36mextract_text\u001b[0;34m(html_file, return_type)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHTMLTree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mreturn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'main'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'websites/www.louissachar.com/Wayside.htm'"
     ]
    }
   ],
   "source": [
    "def extract_text(html_file, return_type):\n",
    "    tree = HTMLTree.parse_from_bytes(open(html_file, 'rb').read(), detect_encoding(open(html_file, 'rb').read()))\n",
    "    if return_type == 'title':\n",
    "        return tree.head.text\n",
    "    elif return_type == 'main':\n",
    "        return tree.head.text + ' ' + extract_plain_text(tree, main_content=True)\n",
    "    return tree.body.text\n",
    "\n",
    "assert extract_text('websites/www.louissachar.com/Wayside.htm', 'title') == '\\nLouis Sachar —  Wayside School Book Series\\n\\n'\n",
    "assert extract_text('websites/www.louissachar.com/Wayside.htm', 'main').startswith(\"\\nLouis Sachar —  Wayside School Book Series\\n\\n \\n\\nTo purchase any of Louis's books visit your local bookseller or make your selection online:\\n\\nSideways Stories from\\nWayside School\\n\\n\\xa0\\n\\nWayside School\\nis Falling Down\\n\\n\\xa0\\n\\nWayside School\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question(entry):\n",
    "    return entry['title'].split(']')[-1] + '. ' + entry['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_normalization():\n",
    "    def stem():\n",
    "        return True\n",
    "\n",
    "    def stop():\n",
    "        return True\n",
    "\n",
    "def construct_instance_for_model_training(entry, text_type):\n",
    "    target_texts = []\n",
    "    question = extract_question(entry)\n",
    "    for link in get_htmls(entry).values():\n",
    "        target_texts.append(extract_text(link, text_type))\n",
    "    input_data = [json.dumps({\"queries\": target_texts, \"doc\":{\"title\": question}})]\n",
    "    return list(calculate_doc_term_recall(input_data, text_normalization()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('training-data-main-06-07-2023.jsonl') as f:\n",
    "    for l in f:\n",
    "        data += [json.loads(l)]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_scores(entry):\n",
    "    instance = entry['training_samples']['instances'][0]\n",
    "    word_scores = instance['term_recall']\n",
    "    text = instance['doc']['title']\n",
    "    \n",
    "    sentences = [sent.text.strip() for sent in nlp(text).sents] \n",
    "\n",
    "    sentence_scores = {i: 0 for i in sentences}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        num_words = len(sentence.split())\n",
    "\n",
    "        for word in word_scores.keys():\n",
    "            if word in sentence:\n",
    "                sentence_scores[sentence] += word_scores[word]\n",
    "\n",
    "        # Normalizing the score based on sentence length\n",
    "        if num_words > 0:\n",
    "            sentence_scores[sentence] = round(sentence_scores[sentence]/num_words, 3)\n",
    "\n",
    "    return sentence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_k_sentences(entry, k):\n",
    "    sentences_for_entry = sentence_scores(entry)\n",
    "    return sorted(list(sentences_for_entry.keys()),key=lambda i: sentences_for_entry[i], reverse = True)[:k]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_p_percent_sentences(entry, p):\n",
    "    scores = sentence_scores(entry)\n",
    "    sorted_sentences = sorted(scores,key=lambda i: scores[i], reverse = True)\n",
    "    cut = int(len(scores) * p/100)\n",
    "    top_sentences = sorted_sentences[:cut]\n",
    "    return {sentence: scores[sentence] for sentence in top_sentences}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_bottom_x_sentences(entry, x):\n",
    "    scores = sentence_scores(entry)\n",
    "    sorted_sentences = sorted(scores,key=lambda i: scores[i], reverse = True)\n",
    "    top_sentences = sorted_sentences[:x]\n",
    "    bot_sentences = sorted_sentences[-x:]\n",
    "    sentences = top_sentences + bot_sentences\n",
    "    return {sentence: scores[sentence] for sentence in sentences}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Guy from a tribe gets separated from his family and gets chased by a pack of wolves.': 0.294,\n",
       " \"Im trying to remember everything as detailed as possible and here's the plot\\n\\nThere was this movie i forgot where there was this guy that got chased by a pack of wolves and he climbs up a tree.\": 0.184,\n",
       " 'One wolf clings on his foot injuring him so hi grabs his spear to stab one of the wolves.': 0.316,\n",
       " 'The wolves then scamper away except for the injured one.': 0.4,\n",
       " 'Days pass and he goes down the tree and thinks of killing the injured wolf': 0.467,\n",
       " 'but he wasnt able to kill it.': 0.143,\n",
       " 'Instead he brought it to a cave where the guy tends for him and the wolf which eventually ended up taming the wolf.': 0.217,\n",
       " 'They both got healed and eventually left the cave.': 0.444,\n",
       " 'The guy tries to drive away the wolf but eventually assists one another.': 0.385,\n",
       " \"They eventually find this hut with a dead guy and the protagonist gets the dead guy's bow from him and they proceed to wander.\": 0.25,\n",
       " 'The wolf then reuinites with its pack members and leaves the guy.': 0.333,\n",
       " 'The guy wanders around and almost freezes to death but he saw the pack of wolves eating a carcass.': 0.316,\n",
       " 'He tries to call out for his wolf friend and falls into a frozen lake.': 0.333,\n",
       " \"The wolf tries to help him abd eventually, using the guy's improvised knife, the guy was able to crawl out of the frozen lake.\": 0.25,\n",
       " 'They then proceed to wander more and encounter the pack of wolves once more.': 0.143,\n",
       " 'They were trying to attack the guy (?)': 0.25,\n",
       " 'But they were able to escape by hiding into another cave.': 0.182,\n",
       " 'Little did they know that there was a saber tooth (or panther? )': 0.154,\n",
       " 'living in that cave and attacks them.': 0.571,\n",
       " 'The wolf protects him and gets gravely injured.': 0.5,\n",
       " 'He then tends for the wolf once more...': 0.125,\n",
       " 'And thats how much i can remember.': 0.143,\n",
       " 'I watched this film a few weeks ago on a public transport and cant remember the title.': 0.235}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_scores(data[60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['American TV show telling you how to survive disaster situations.',\n",
       " 'It only aired a few episodes in my country (NZ) a few years ago but I found it super interesting.',\n",
       " 'Each episode would feature a small case situation and a large scale one, eg.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_top_k_sentences(data[7], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Show about how to survive disaster situations.': 0.429,\n",
       " 'American TV show telling you how to survive disaster situations.': 0.4,\n",
       " 'Hour long episodes I think.': 0.4,\n",
       " 'Each episode would feature a small case situation and a large scale one, eg.': 0.286}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_top_p_percent_sentences(data[7],50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Show about how to survive disaster situations.': 0.429,\n",
       " 'American TV show telling you how to survive disaster situations.': 0.4}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_top_p_percent_sentences(data[7],25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Show about how to survive disaster situations.', 'American TV show telling you how to survive disaster situations.', 'There was an episode about a Pandemic, another about a home invasion.', 'Hosted by a guy.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Show about how to survive disaster situations.': 0.429,\n",
       " 'American TV show telling you how to survive disaster situations.': 0.4,\n",
       " 'There was an episode about a Pandemic, another about a home invasion.': 0.083,\n",
       " 'Hosted by a guy.': 0.0}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_top_bottom_x_sentences(data[7],2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "training_method = 'title'\n",
    "with open('training-data-title-06-07-2023.jsonl', 'w') as f:\n",
    "    for entry in tqdm(entries):\n",
    "        entry = deepcopy(entry)\n",
    "        training_samples = []\n",
    "        try:\n",
    "            for i in construct_instance_for_model_training(entry, training_method):\n",
    "                if i['term_recall']:\n",
    "                    training_samples += [i]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        entry['training_samples'] = {'method': training_method, 'instances': training_samples}\n",
    "    \n",
    "        if training_samples:\n",
    "            f.write(json.dumps(entry) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_instance_for_model_training(entries[5], 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = [{'doc': {'id': 1,\n",
    "   'position': 1,\n",
    "   'title': \" A woman birthing monsters/aliens. I was re-watching Prometheus and during the c-section scene in the giant med-machine I had a little flashback to a movie or series (not sure) I saw as a child of a human woman giving birth to either monsters, mutants or aliens. It's not much to go on but the only thing i remember are a long medical table,  an orange light being cast on the table and a woman popping out what at the time I believed were aliens? It must have been an 80's early 90' production. \\nThanks in advance :)\\n\\nEdit:  Everything had a dark atmosphere, like it was happening at night or in a dark room.\"},\n",
    "  'term_recall': {'birth': 1.0,\n",
    "   'birthing': 1.0,\n",
    "   'c': 1.0,\n",
    "   'scene': 1.0,\n",
    "   'time': 1.0}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = instance[0]['doc']['title']\n",
    "\n",
    "list = instance[0]['term_recall'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in tqdm(entries):\n",
    "    download_page(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./reddit-tomt-submissions.jsonl | shuf | head -100000 > json-tomt-sample.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat json-tomt-sample.jsonl | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"./json-tomt-sample.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'] = df.apply(lambda i: extract_category(i), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_category(tomt):\n",
    "    \n",
    "    return tomt.to_dict()['title'].split('[')[-1].split(']')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['category'].str.contains('site')].category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['category'].str.contains('page')].category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Initialize the figure with a logarithmic x axis\n",
    "f, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.set_yscale(\"log\")\n",
    "sns.boxplot(df.ups)\n",
    "# observation: zero or ten or more upvotes are outlayers, hence we take questions with 1-9 upvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure with a logarithmic x axis\n",
    "f, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.set_yscale(\"log\")\n",
    "sns.boxplot(df.downs)\n",
    "# observation:questions with one downvote are already outlayers, hence we only take questions with zero downvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the figure with a logarithmic x axis\n",
    "f, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.set_yscale(\"log\")\n",
    "sns.boxplot(df.num_comments)\n",
    "# observation:questions with zero or 13 comments are already outlayers, hence we only take questions with zero downvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.num_comments.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.num_comments.quantile(q=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select topics\n",
    "\n",
    "#define conditions\n",
    "c1 = df[\"solved_utc\"] != \"\"\n",
    "c2 = (df[\"ups\"]>0) & (df[\"ups\"]<10)\n",
    "c3 = df[\"downs\"]==0\n",
    "c4 = (df[\"num_comments\"] > 0) & (df[\"num_comments\"]<13)\n",
    "\n",
    "#apply conditions to dataframe\n",
    "topics = df[c1 & c2 & c3 & c4]\n",
    "\n",
    "#select samples by category\n",
    "books = topics[topics[\"category\"].isin([\"BOOK\", \"Book\"])].sample(250)\n",
    "movies = topics[topics[\"category\"].isin([\"MOVIE\", \"Movie\", \"movie\"])].sample(250)\n",
    "websites = topics[topics[\"category\"].isin([\"Website\", \"website\"])] #less then 250 elements, so all are taken\n",
    "songs = topics[topics[\"category\"].isin([\"SONG\", \"Song\", \"song\", \"MUSIC\", \"Music\"])].sample(250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books.to_json(\"./books.json\", orient = \"records\", lines = True)\n",
    "movies.to_json(\"./movies.json\", orient = \"records\", lines = True)\n",
    "websites.to_json(\"./websites.json\", orient = \"records\", lines = True)\n",
    "songs.to_json(\"./songs.json\", orient = \"records\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(books.shape) #(272, 127)\n",
    "print(movies.shape) #(1029, 127)\n",
    "print(websites.shape) #(110, 127)\n",
    "print(songs.shape) #(1385, 127)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
